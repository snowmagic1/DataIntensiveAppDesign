each log structured storage segment is a sequence of key value pairs, these pairs appear in an order that they were written, values later in the log take precedence over values for the same key earlier in the file, apart from that, the order of key-value pairs doesn't matter

- merging segments is simple, even if the file is bigger than the avaliable memory
- in order to find the value for a particular key, you don't need to keep an index of all the keys in memory
  you still need some index to tell you the offsets for some keys, but it can be sparse. one key for every k bytes is        sufficient, because a few k can be scanned efficiently
- since read request need to scan over several key value paris anyway, it's possible to group those records into a block and compress it before writting to disk

## constructing and maintaining sstable
- when a write comes in, add it to a in memory balanced tree structure, this tree structure sometimes is called a memtable
- when the memtable gets bigger than some threshold, usually a few mb, write it out to disk as a sstable file, this can be done efficiently because the tree structure already maintains the key value pairs sorted by key. while the sstable is being writeen out to disk, writes can continue on a new memtable instance
- in order to serve a read request, first find the key in memtable, then in the most recent sstable, then in the next older sstable
- from time to time, run a merging and compacting process in the background to combine segment files, and to discard overwritten or deleted record

the schema works very well, it only suffers from one problem, if the database crashes, the most recent writes are log(which are in memtable and not yet written out to disk). in order to avoid that problem, we can keep another log on disk to which each write is appened immediately, that log is not in a sorted order but that doesn't matter, because its only purpose is to restore the memtable after a crash

## perf optimization
bloom filter
size-tiered, leveled compaction
