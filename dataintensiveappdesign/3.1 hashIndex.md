well suited for situations where the value for each key is frequently updated
in such workload, there're a lot writes but not many distint keys - you have a large number of writes per key but it's feasible to keep all the keys in memory
compaction & merge
if you want to delete a key and its associated value, you have to append a special deletion record to the data file(sometimes called a tombstone), when the segments are merged, the tombstone tells the merge process to discard any pervious value for the deleted key.

- appending and segments merge are sequencial write ops, which is in general much faster than random writes
- concurrent and crash recover are simplier if all the data files are append only or immutable, for example, you don't have to worry about the case where a crash happened while a value is being overwritter, leaving you with the data file that contains part of the old data and part of the new data splice together
- merging old segments avoids the problem of segments becoming framented over time

limitations
- the hash table has to fit in memory, so if you have a large number of keys, you are out of luck.
- range query is not efficient
