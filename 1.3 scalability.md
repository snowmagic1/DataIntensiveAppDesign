## describing load
twitter, avg/peak read write qps, fan out factor

## describing performance
once we have described load, we can investigate what happens when load increases, we can look at it in 2 ways
- when one load parameter increases and keep the system resource unchanged, how is the performance of the system affected
- when one load parameter increases how much resources do you need to increase to keep the performance unchanged

latency and response time are usually used unexchangably, but they are not the same. response time is what the client sees, besides the actual time to process the request, it also includes network delays and queuing delays. latency is the duration that a request is waiting to be handled.

usually it's better to use percentile, if you take your list of response times, sort them from fastest to slowest, the median is the halfway point.

queneing delay accounts for a large part of the response time at high percentile, because the server can only process a small number of things in parellal(by the number of cpu cores), it only takes a small number of slow requests to hold up the processing of the subsequent requests, an effect sometimes known as head of line blocking, even if those subsequent requests are fast to process, the client will still see a slow overall resposne time, due to the time waiting for the prior requests to complete, due to this effect, it's important to measure performance on client side.

