if you have been working in software engineering in recent years, especially in server or backend systems, you have probably been bombarded by plenty of buzzwords like nosql, cap etc.
in the last decade, we have seen many intersting developments in databases, distibuted systems and in the way we build applications on top of them, there're various driving forces for these developments
- big internet companies are handling huge volume of data and traffic, which force them to create new tools that enable them handling such scale efficiently
- bussinesses need to be agile, test hypothisises cheaply and respond to new market insights rapidly by making the development circle short and data model flexible
- free and open source software have been very successful, and now is prefered to commercial and in-house softwares in many enviroments
- CPU clock speed are barely increasing, but multi-core processor are stardard and network are getting faster, which means parallisism is only going to increase
- even you are working on small teams, you can now create applications that are distributed on many machines and across multiple geography regions thanks to iaas eg. aws
- many services are expected to be highly avaliable, down time due to outage or maitainence are increasingly unacceptable

data intensive applications are pushing the boundary of what's possible by making use of these developments, we call an application data intensive if data is its primary challenge - the volume, the complexity and the speed at which it is chaning. as opposite to compute intensive application, where CPU circles are their primary bottleneck

the tools and technolegies that help data intensive applications are rapidly 
