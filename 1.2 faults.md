everyone has an intuitive idea of something to be reliable and unreliable, for softwares typical expectations include
- the application perform the function the user expect
- it can tolerant user to make mistake, or using it in a unexpected way
- it performance is good enough for the required use case, under the expected load and user volume
- the system prevents unauthorized access

if all of the things above together means working correctly, then roughly reliablility means the application continues to working correctly even when things go wrong.

the things that can go wrong are called faults, and systems that anticipate faults and can cope with them are called fault tolerance or resillent.

note that a fault is not the same as a failure, a fault is usually defined as a component is deviating from its spec, whereas a failure means the system as a whole stop providing the service to the client, it is impossible to reduce the probability of fault to 0, therefore it's usually best to design the fault-tolerance mechinism to prevent faults from causing failures.

Counterintuitively, in such fault tolerant system, it can make sense to increase the fault rate by triggering them deliberately, for example, randomly killing process without warning. many critical bugs are actually due to poor error handling, by deliberately inducing fault, you continious ensure the fault tolerent mechinery is excersiced and tested, which will increase the confidence that the fault will be handled correctly when they occurred naturely.

although we generaly prefer fault tolerance over fault preventing, but there're cases fault preventing is better than fault tolerance, that're the cases that security matters.
